{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from helpers import *\n",
    "from Model import *\n",
    "from cross_val import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape  (250000, 30)\n",
      "Test set shape  (568238, 30)\n"
     ]
    }
   ],
   "source": [
    "#[yb_t, input_data_t, ids_t, yb_test, input_data_test, ids_test] = pickle.load(open( \"dat.p\", \"rb\" ))\n",
    "# Import full data\n",
    "(yb_t, input_data_t, ids_t) = load_csv_data('data/train.csv', sub_sample = False)\n",
    "(yb_test, input_data_test, ids_test) = load_csv_data('data/test.csv',sub_sample = False)\n",
    "\n",
    "# Replace missing data with nan\n",
    "tx_tr_clean = clean_data(input_data_t)\n",
    "y_tr_clean = clean_data(yb_t)\n",
    "tx_te_clean = clean_data(input_data_test)\n",
    "y_te_clean = clean_data(yb_test)\n",
    "\n",
    "print('Train set shape ', tx_tr_clean.shape)\n",
    "print('Test set shape ', tx_te_clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tr_train.shape =  (250000, 30)\n",
      "input_tr_train.shape =  (250000, 29)\n",
      "input_tr_train.shape =  (250000, 33)\n",
      "tx_tr_cat.shape =  (250000, 33)\n",
      "tx_tr_cat[:,28] =  [ 113.497   46.226   44.251 ...,   41.992    0.       0.   ]\n",
      "tx_tr_cat[:,29] =  [ 0.  0.  0. ...,  0.  1.  1.]\n",
      "tx_tr_cat[:,30] =  [ 0.  1.  1. ...,  1.  0.  0.]\n",
      "tx_tr_cat[:,31] =  [ 1.  0.  0. ...,  0.  0.  0.]\n",
      "tx_tr_cat[:,32] =  [ 0.  0.  0. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Extend categorical variable into three features\n",
    "def extendCategorical(input_tx_train, input_tx_test):\n",
    "    mask_tr_0 = (input_tx_train[:,22] == 0).astype(int).reshape((-1, 1))\n",
    "    mask_tr_1 = (input_tx_train[:,22] == 1).astype(int).reshape((-1, 1))\n",
    "    mask_tr_2 = (input_tx_train[:,22] == 2).astype(int).reshape((-1, 1))\n",
    "    mask_tr_3 = (input_tx_train[:,22] == 3).astype(int).reshape((-1, 1))\n",
    "    mask_te_0 = (input_tx_test[:,22] == 0).astype(int).reshape((-1, 1))\n",
    "    mask_te_1 = (input_tx_test[:,22] == 1).astype(int).reshape((-1, 1))\n",
    "    mask_te_2 = (input_tx_test[:,22] == 2).astype(int).reshape((-1, 1))\n",
    "    mask_te_3 = (input_tx_test[:,22] == 3).astype(int).reshape((-1, 1))\n",
    "    print(\"input_tr_train.shape = \", input_tx_train.shape)\n",
    "    input_tx_train = np.delete(input_tx_train, 22, 1)\n",
    "    input_tx_test = np.delete(input_tx_test, 22, 1)\n",
    "    print(\"input_tr_train.shape = \", input_tx_train.shape)\n",
    "    input_tx_train = np.concatenate((input_tx_train, mask_tr_0, mask_tr_1, mask_tr_2, mask_tr_3), axis = 1)\n",
    "    input_tx_test = np.concatenate((input_tx_test, mask_te_0, mask_te_1, mask_te_2, mask_te_3), axis = 1)\n",
    "    print(\"input_tr_train.shape = \", input_tx_train.shape)\n",
    "    return input_tx_train, input_tx_test\n",
    "\n",
    "tx_tr_cat, tx_te_cat = extendCategorical(tx_tr_clean, tx_te_clean)\n",
    "print(\"tx_tr_cat.shape = \", tx_tr_cat.shape)\n",
    "print(\"tx_tr_cat[:,28] = \", tx_tr_cat[:, 28])\n",
    "print(\"tx_tr_cat[:,29] = \", tx_tr_cat[:, 29])\n",
    "print(\"tx_tr_cat[:,30] = \", tx_tr_cat[:, 30])\n",
    "print(\"tx_tr_cat[:,31] = \", tx_tr_cat[:, 31])\n",
    "print(\"tx_tr_cat[:,32] = \", tx_tr_cat[:, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standardize data\n",
    "tx_tr, mean_tx_tr, std_tx_tr = standardize(tx_tr_cat)\n",
    "\n",
    "# Test data should be standardize wt respect to the mean and standard deviation of the training set\n",
    "tx_te = (tx_te_cat - mean_tx_tr)/std_tx_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Claire\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: Mean of empty slice\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Replacing missing values by their respective mean for each feature (computing for the TRAINING set only!)\n",
    "def replaceByMeanPerFeature(input_tx_train, input_y_train, input_tx_test, input_y_test):\n",
    "    train_mean = np.nanmean(input_tx_train, axis=0) # Computes the mean per column without considering nan value\n",
    "    for ind, mean in enumerate(train_mean):\n",
    "        mask_tmp = np.isnan(input_tx_train[:,ind])\n",
    "        input_tx_train[mask_tmp, ind] = mean\n",
    "        mask_tmp = np.isnan(input_tx_test[:,ind])\n",
    "        input_tx_test[mask_tmp, ind] = mean\n",
    "    y_train = input_y_train\n",
    "    y_test = input_y_test\n",
    "    return input_tx_train, y_train, input_tx_test, y_test\n",
    "\n",
    "tx_tr, y_tr, tx_te, y_te = replaceByMeanPerFeature(tx_tr, y_tr_clean, tx_te, y_te_clean)\n",
    "y_tr = np.array([0 if p<0 else 1 for p in y_tr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    poly = np.ones((len(x), 1))\n",
    "    for deg in range(1, degree+1):\n",
    "        poly = np.c_[poly, np.power(x, deg)]\n",
    "    return poly[:, 1:]\n",
    "\n",
    "def augment_feat(input_tx_train, input_tx_test, degree):\n",
    "    n_features = len(input_tx_train[0])\n",
    "    for ind in range(0, n_features):\n",
    "        input_tx_train = np.c_[input_tx_train, build_poly(input_tx_train[:, ind], degree)]\n",
    "        input_tx_test = np.c_[input_tx_test, build_poly(input_tx_test[:, ind], degree)]\n",
    "    return input_tx_train, input_tx_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit data with Ridge Regression with no feature augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Model import ridge_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation_visualization(lambds, mse_tr, mse_te):\n",
    "    \"\"\"visualization the curves of mse_tr and mse_te.\"\"\"\n",
    "    plt.semilogx(lambds, mse_tr, marker=\".\", color='b', label='train error')\n",
    "    plt.semilogx(lambds, mse_te, marker=\".\", color='r', label='test error')\n",
    "    plt.xlabel(\"lambda\")\n",
    "    plt.ylabel(\"rmse\")\n",
    "    plt.title(\"cross validation\")\n",
    "    plt.legend(loc=2)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"cross_validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (62500,) (62500,10000) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-769396c94de0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mcross_validation_visualization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlambdas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrmse_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrmse_te\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mcross_validation_demo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-769396c94de0>\u001b[0m in \u001b[0;36mcross_validation_demo\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m# define model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_regression_SGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mlosses_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_fold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mtmp_tr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosses_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mtmp_te\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosses_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Claire\\Documents\\EPFL\\Higgs\\cross_val.py\u001b[0m in \u001b[0;36mcross_validation\u001b[1;34m(model, y, x, k_fold, seed)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_tr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0merr_te\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_te\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_te\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m         \u001b[0merr_tr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Claire\\Documents\\EPFL\\Higgs\\Model.py\u001b[0m in \u001b[0;36mcompute_error\u001b[1;34m(y, tx, w)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcompute_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcalculate_mse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (62500,) (62500,10000) "
     ]
    }
   ],
   "source": [
    "def cross_validation_demo():\n",
    "    seed = 1\n",
    "    degree = 1\n",
    "    k_fold = 4\n",
    "    lambdas = np.logspace(-4, 3, 30)\n",
    "    \n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y_tr, k_fold, seed)\n",
    "    \n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    \n",
    "    # cross validation\n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        # define model\n",
    "        model = logistic_regression_SGD(gamma = lambda_, max_iters = 1)\n",
    "        losses_train, losses_test = cross_validation(model, y_tr, tx_tr, k_fold)\n",
    "        tmp_tr = np.mean(losses_train)\n",
    "        tmp_te = np.mean(losses_test)\n",
    "        rmse_tr.append(tmp_tr)\n",
    "        rmse_te.append(tmp_te)\n",
    "    cross_validation_visualization(lambdas, rmse_tr, rmse_te)\n",
    "\n",
    "cross_validation_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit data with Ridge Regression with data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tx_tr_in = tx_tr\n",
    "tx_te_in = tx_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_tr, tx_te = augment_feat(tx_tr_in, tx_te_in, 3)\n",
    "print(\"tx_tr.shape = \", tx_tr.shape)\n",
    "print(\"tx_te_shape = \", tx_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_demo():\n",
    "    seed = 1\n",
    "    degree = 1\n",
    "    k_fold = 4\n",
    "    lambdas = np.logspace(-6, 5, 30)\n",
    "    \n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y_tr, k_fold, seed)\n",
    "    \n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    \n",
    "    # cross validation\n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        # define model\n",
    "        model = ridge_regression(lambda_)\n",
    "        losses_train, losses_test = cross_validation(model, y_tr, tx_tr, k_fold)\n",
    "        tmp_tr = np.mean(losses_train)\n",
    "        tmp_te = np.mean(losses_test)\n",
    "        rmse_tr.append(tmp_tr)\n",
    "        rmse_te.append(tmp_te)\n",
    "    print(\"min_rmse_te = \", np.min(rmse_te))\n",
    "    cross_validation_visualization(lambdas, rmse_tr, rmse_te)\n",
    "\n",
    "cross_validation_demo()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit data with Ridge Regression and augmenting features using square root of absolute value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_poly_log(x):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    poly_log = np.ones((len(x), 1))\n",
    "    poly_log = np.c_[poly_log, np.sqrt(np.abs(x))]\n",
    "    return poly_log[:, 1:]\n",
    "\n",
    "def augment_feat_log(input_tx_train, input_tx_test):\n",
    "    n_features = len(input_tx_train[0])\n",
    "    for ind in range(0, n_features):\n",
    "        input_tx_train = np.c_[input_tx_train, build_poly_log(input_tx_train[:, ind])]\n",
    "        input_tx_test = np.c_[input_tx_test, build_poly_log(input_tx_test[:, ind])]\n",
    "    return input_tx_train, input_tx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_tr, tx_te = augment_feat_log(tx_tr_in, tx_te_in)\n",
    "print(\"tx_tr.shape = \", tx_tr.shape)\n",
    "print(\"tx_te_shape = \", tx_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation_demo():\n",
    "    seed = 1\n",
    "    degree = 1\n",
    "    k_fold = 4\n",
    "    lambdas = np.logspace(-6, 5, 30)\n",
    "    \n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y_tr, k_fold, seed)\n",
    "    \n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    \n",
    "    # cross validation\n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        # define model\n",
    "        model = ridge_regression(lambda_)\n",
    "        losses_train, losses_test = cross_validation(model, y_tr, tx_tr, k_fold)\n",
    "        tmp_tr = np.mean(losses_train)\n",
    "        tmp_te = np.mean(losses_test)\n",
    "        rmse_tr.append(tmp_tr)\n",
    "        rmse_te.append(tmp_te)\n",
    "    print(\"min_rmse_te = \", np.min(rmse_te))\n",
    "    cross_validation_visualization(lambdas, rmse_tr, rmse_te)\n",
    "\n",
    "cross_validation_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_tr = np.array([0 if p<0 else 1 for p in y_tr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = ridge_regression(1e-5)\n",
    "model.fit(y_tr, tx_tr)\n",
    "y_pred_test = model.predict(tx_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_bin = np.array([-1 if p<0.5 else 1 for p in y_pred_test])\n",
    "np.sum(y_pred_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_csv_submission(ids_test, y_pred_bin, 'sub3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Fit data with Ridge Regression and augmenting features using log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
